{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Availability of CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, time\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "#Set location of the torch tensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Availability of GPU:',use_cuda)\n",
    "if use_cuda: torch_ft = torch.cuda.FloatTensor\n",
    "else: torch_ft = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data\n",
    "!wget http://opendata.cern.ch/record/328/files/atlas-higgs-challenge-2014-v2.csv.gz\n",
    "!gunzip atlas-higgs-challenge-2014-v2.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data (the input file contains ~820k different events)\n",
    "df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnecessary variables\n",
    "df = df.drop('EventId',axis=1).drop('Weight',axis=1).drop('KaggleSet',axis=1).drop('KaggleWeight',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training, validation and test sets, at the beginning use small dataset for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_df = df[:500000]\n",
    "valid_df = df[500000:650000]\n",
    "test_df = df[650000:]\n",
    "\n",
    "#training_df = df[:5000]\n",
    "#valid_df = df[5000:6500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "In exercise 1 we used [fastai.tabular](https://docs.fast.ai/tabular.data.html) module:\n",
    "```python\n",
    "from fastai.tabular import TabularDataBunch\n",
    "data = TabularDataBunch.from_df(path, df:DataFrame, dep_var:str, valid_idx:Collection[int], \n",
    "                         procs:Optional[Collection[TabularProc]]=None, cat_names:OptStrList=None, \n",
    "                         cont_names:OptStrList=None, classes:Collection[T_co]=None, test_df=None, \n",
    "                         bs:int=64, val_bs:int=None, num_workers:int=4, \n",
    "                         dl_tfms:Optional[Collection[Callable]]=None, device:device=None, \n",
    "                         collate_fn:Callable='data_collate', no_check:bool=False) â†’ DataBunch\n",
    "```\n",
    "Today we will write our own function to generate mini-batches. It is done in two steps:\n",
    "- creating a DataSet\n",
    "- creating a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'Label'\n",
    "batch_size = len(training_df)\n",
    "\n",
    "#convert pandas dataframe to torch tensor\n",
    "train_target = torch_ft(training_df[dep_var].replace({'s':1,'b':0}).values.astype(np.float32))\n",
    "train_dataset = torch_ft(training_df.drop(dep_var, axis = 1).values.astype(np.float32)) \n",
    "\n",
    "#use torch tensor as input to mini-batch generator\n",
    "train_dataset = TensorDataset(train_dataset, train_target) \n",
    "train_generator = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "#repeat for the validation set, here use full batch for evaluation\n",
    "valid_target = torch_ft(valid_df[dep_var].replace({'s':1,'b':0}).values.astype(np.float32))\n",
    "valid_dataset = torch_ft(valid_df.drop(dep_var, axis = 1).values.astype(np.float32)) \n",
    "\n",
    "valid_dataset = TensorDataset(valid_dataset, valid_target) \n",
    "valid_generator = DataLoader(dataset = valid_dataset, batch_size = len(valid_df), shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner (train function)\n",
    "\n",
    "In ex. 1 we used `Learner()` from [basic_train](https://docs.fast.ai/basic_train.html#Learner) library\n",
    "```python \n",
    "learn = Learner(data, net, loss_func = nn.CrossEntropyLoss())\n",
    "```\n",
    "Here we will write a Learner with the `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data_loader, model, optimizer, criterion, valid_loader = None):\n",
    "        \n",
    "        self.data_loader = data_loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.valid_loader = valid_loader\n",
    "        if torch.cuda.is_available() and not next(model.parameters()).is_cuda:\n",
    "            print('copy the model to GPU')\n",
    "            model.to(torch.device(\"cuda\"))\n",
    "            \n",
    "\n",
    "    def fit(self, epochs = 1, cache={'loss':[]}):\n",
    "        \n",
    "        tic = time.time()\n",
    "        \n",
    "        # Iteration over the epochs\n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            #Loop over the mini-batches\n",
    "            for i, (batch_input, batch_target) in enumerate(self.data_loader):\n",
    "\n",
    "                #evaluate the model, and compute the gradients\n",
    "                batch_output = self.model(batch_input)\n",
    "                loss=self.criterion(batch_output, batch_target)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            cache['loss'].append(loss.item())\n",
    "\n",
    "            #evaluate validation loss:\n",
    "            if self.valid_loader:\n",
    "                with torch.no_grad():\n",
    "                    valid_input, valid_target = next(iter(self.valid_loader))\n",
    "                    valid_output = self.model(valid_input)\n",
    "                    vloss = criterion(valid_output, valid_target)\n",
    "                    cache['val_loss'].append(vloss.item())\n",
    "\n",
    "                printProgressBar(epoch, epochs, [loss.item(),vloss.item()])\n",
    "            else: printProgressBar(epoch, epochs, [loss.item()])\n",
    "        toc = time.time()\n",
    "        print('total time: %2.2f sec' %(toc-tic))\n",
    "        return cache\n",
    "\n",
    "\n",
    "def printProgressBar (iteration, total, losses = [], decimals = 1, length = 50):\n",
    "    total = total - 1 #since usually we start from 0 till n-1\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = '#' * filledLength + '-' * (length - filledLength)\n",
    "    if len(losses)==1:\n",
    "        print('\\rprogress |%s| %s%% loss - %s' % (bar, percent, str(losses[0])), end = '\\r')\n",
    "    else:\n",
    "        print('\\rprogress |%s| %s%% loss - %2.5f | validation - %2.5f' % \n",
    "              (bar, percent, float(losses[0]), float(losses[1])), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "        \n",
    "def CreateCash(model):\n",
    "    cache={}\n",
    "    for c in (['loss']+['val_loss']+[*model.state_dict().keys()]):\n",
    "        cache[c]=[]\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "\n",
    "Let's construct a simple NN, for the propose we will use 4-Layer NN with H = [100,50,10]\n",
    "![4-Layer NN](images/4L_NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 30                # dimentions of the input layer\n",
    "H = [100, 50, 10]             # dimentions of the hidden layers\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim,H[0])\n",
    "        self.layer2 = nn.Linear(H[0],H[1])\n",
    "        self.layer3 = nn.Linear(H[1],H[2])\n",
    "        self.layer4 = nn.Linear(H[2],1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #note: we dont use categories, but we need to set it up in the model\n",
    "        out  = self.layer1(x)\n",
    "        out  = self.activation(out)\n",
    "        out  = self.layer2(out)\n",
    "        out  = self.activation(out)\n",
    "        out  = self.layer3(out)\n",
    "        out  = self.activation(out)\n",
    "        out  = self.layer4(out)\n",
    "        out  = self.sigmoid(out)\n",
    "        \n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "to run the train function we need to define the loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "cache = CreateCash(net)\n",
    "criterion = getattr(nn.functional, 'binary_cross_entropy') #advanced user would like to try binary_cross_entropy_with_logits\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-3)\n",
    "learn = Learner(train_generator, net, optimizer, criterion, valid_loader = valid_generator)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set!! we can start to train our model! Instead of running:\n",
    "```python \n",
    "learn.fit(epochs = 2, lr = 5e-3)\n",
    "```\n",
    "we will run our `Learner.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress |#####---------------------------------------------| 11.1% loss - 14.81277 | validation - 14.66815\r"
     ]
    }
   ],
   "source": [
    "cache = learn.fit(10, cache)\n",
    "plt.plot(cache['loss'],label='training loss')\n",
    "plt.plot(cache['val_loss'],label='validation loss')\n",
    "plt.legend(); plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, y = next(iter(valid_generator))\n",
    "    preds = net(inputs)\n",
    "    \n",
    "preds = preds.data.numpy()\n",
    "preds_sig = preds[y.numpy()==0]\n",
    "preds_bkg = preds[y.numpy()==1]\n",
    "\n",
    "bins = np.linspace(0,1,30)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.hist(preds_bkg,bins=bins,density=True,label='background')\n",
    "ax.hist(preds_sig,bins=bins,density=True,histtype='step',linewidth=3,label='signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
