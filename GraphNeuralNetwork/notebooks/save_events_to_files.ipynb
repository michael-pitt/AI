{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading docker image\n",
    "\n",
    "if running on a public cluster, do the following: Download the data to `INPUTFOLDER/train_sample_full`\n",
    "\n",
    "* start docker\n",
    "```bash\n",
    "INPUT_DATA=INPUTFOLDER\n",
    "docker run -it -p 8888:8888 --rm -v $(pwd):/home/code -v $INPUT_DATA:/home/data estradevictorantoine/trackml:1.0\n",
    "jupyter notebook --ip 0.0.0.0 --no-browser --allow-root\n",
    "```\n",
    "* open a tunnel\n",
    "in a new shell open a tunnel\n",
    "```bash\n",
    "ssh -N -f -L localhost:7008:localhost:8888 adress.of.cluster\n",
    "```\n",
    "\n",
    "* in chrome open the notebook:\n",
    "in chrome: http://localhost:7008/, the notebook will be in `code` folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate files with inputs\n",
    "\n",
    "This notebook used to generate graphs and store them as *.npz files\n",
    "\n",
    "This notebook is working from the docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trackml.dataset import load_dataset\n",
    "PATH_TO_DATA = \"/home/data/train_sample_full\"\n",
    "#PATH_TO_DATA = \"/home/data/train_sample_single\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input selection:\n",
    "\n",
    "Generate inputs from the file using preprocess code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Graph = namedtuple('Graph', ['X', 'Is', 'y'])\n",
    "from preprocess import preprocess\n",
    "def graph_to_sparse(graph):\n",
    "    return dict(X=graph.X, y=graph.y, Is=graph.Is)\n",
    "def get_size(graph):\n",
    "    size = 0\n",
    "    for fld in graph._fields:\n",
    "        size += getattr(graph, fld).nbytes\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select input features, and $\\eta$ range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['x', 'y', 'z', 'phi', 'eta', 'r']\n",
    "#eta_range = np.array([-1.5, -0.75, -0.5,-0.25, 0.0, 0.25, 0.5, 0.75, 1.5 ])\n",
    "eta_range = np.array([-1.5, 1.5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the folder with the output data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/data/graph_full_6var_250MeV_150mmz0_1eta/'\n",
    "!rm -f /home/data/graph_full_6var_250MeV_150mmz0_1eta/*npz\n",
    "!mkdir /home/data/graph_full_6var_250MeV_150mmz0_1eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since using only the barrel, set `getLayer` function that for each hit will set it layer number (total 10 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLayer(volume_id, layer_id):\n",
    "    if volume_id==8:\n",
    "        return layer_id//2\n",
    "    elif volume_id==13:\n",
    "        return layer_id//2 + 4\n",
    "    elif volume_id==17:\n",
    "        return layer_id//2 + 8\n",
    "    else:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_dataset(PATH_TO_DATA, parts=['hits', 'cells', 'truth', 'particles'])\n",
    "keys = ['hit_id','x','y','z','particle_id','volume_id','layer_id']\n",
    "#loop over all files in the folder\n",
    "for event_id, hits, cells, truth, particles in data:\n",
    "    \n",
    "    print('evaluate and store event',event_id,' with',hits.shape[0],'hits')\n",
    "    \n",
    "    r = np.sqrt(hits['x']**2 + hits['y']**2)\n",
    "    theta = np.arctan2(r,hits['z'])\n",
    "    hits['eta'] = -np.log(np.tan(0.5*theta))\n",
    "    hits['layer'] = hits.apply(lambda x: getLayer(x['volume_id'],x['layer_id']), axis=1)\n",
    "\n",
    "    #filter event - remove noise, use only barrel, keep hits associated to tracks with more than 10 hits\n",
    "    new_hits = hits.merge(truth[['hit_id','particle_id']], on='hit_id').copy()\n",
    "    \n",
    "    group_hits = new_hits.groupby(by=['particle_id'])\n",
    "    new_hits = group_hits.filter(lambda x: x['layer'].min() > 0)\n",
    "    new_hits = new_hits.loc[new_hits['particle_id']>0]\n",
    "    print('remove noise and tracks outside the barrel: ',new_hits.shape[0])\n",
    "    \n",
    "    #filter hits to be within the eta region:\n",
    "    eta_cut = (-1.5,1.5);\n",
    "    new_hits = (new_hits.loc[(new_hits['eta']>eta_cut[0]) & (new_hits['eta']<=eta_cut[1])])\n",
    "    print('eta cut ' ,new_hits.shape[0])\n",
    "    \n",
    "    group_hits = new_hits.groupby(by=['particle_id'])\n",
    "    track_idx = group_hits.indices\n",
    "    new_hits = pd.concat([group_hits.get_group(pid).assign(nhits=len(idx)) for pid, idx in track_idx.items()])\n",
    "    new_hits = new_hits.loc[(new_hits['nhits']>9)]\n",
    "    print('n_hits cut' ,new_hits.shape[0])\n",
    "\n",
    "    #reprocess the event using the compiled \"preprocess\" function, compute full graph for entire event (don't split)\n",
    "    print('call reprocess')\n",
    "    list_y, list_X, list_Is, list_hits_id, list_labels = preprocess(new_hits.copy(), eta_range, feature_names)\n",
    "    print('done!')\n",
    "    i = 0\n",
    "    for y, X, Is, hits_id, labels in zip(list_y, list_X, list_Is, list_hits_id, list_labels):\n",
    "        #store the inputs\n",
    "        print(X.shape)\n",
    "        y = y.astype(np.float32)\n",
    "        Is = Is.values\n",
    "        graph = Graph(X,Is,y)\n",
    "        filename = PATH+'/myGraph_event_%d_eta%d.npz'%(event_id,i); i = i+1\n",
    "        np.savez(filename, **graph_to_sparse(graph))\n",
    "        print('graph of size',get_size(graph)/(1024*1024),'MB with ',X.shape[0],'nodes and ',Is.shape[0],'edges \\nsaved in location',filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
