{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data analysis - linear regression.\n",
    "\n",
    "In this section, we will practice basics tasks of curve fitting, using the linear regression approach. \n",
    "\n",
    "First lets load three modules\n",
    "1. numpy - for fast computations with vectors\n",
    "2. matplotlib - we will use this library to plot figures and graphs\n",
    "3. h5py - this is used to save and read data from the `h5` format file (https://en.wikipedia.org/wiki/Hierarchical_Data_Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a data set {x,y}. For this exercise the data is stored in `h5py` format file, which can be read using the next command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/lindata.h5','r')\n",
    "print(\"Keys: %s\" % list(h5f.keys()))\n",
    "x = h5f['x']\n",
    "y = h5f['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later proposes let's convert dimention of data vectors $\\vec{v}$ from $n$ - dim to $n\\times 1$ - dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x old shape = ',x.shape)\n",
    "print('y old shape = ',y.shape)\n",
    "x = np.expand_dims(x,axis=1)\n",
    "y = np.expand_dims(y,axis=1)\n",
    "print('x new shape = ',x.shape)\n",
    "print('y new shape = ',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data, to do it we will use the `matplotlib.pyplot` function which is imported as `plt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y,marker='o', c='k')\n",
    "plt.xlabel('x'); plt.ylabel('y');\n",
    "plt.xlim([0,2.3]); plt.ylim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to \"learn\" the trend in the data and to be able to predict what will be the outcome of a new data point.\n",
    "\n",
    "As one can notice the data are more or less lying on a straight line \n",
    "$$y = ax + b$$\n",
    "\n",
    "This will be our model.\n",
    "What we want to compute is the model parameters $a$ and $b$.\n",
    "\n",
    "How to choose the model parameters? Let's draw few examples for $(a,b) = (0,200)$ and $(a,b) = (4,50)$\n",
    "First we define the model funciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 50\n",
    "x_line = np.linspace(0,100,100)\n",
    "plt.scatter(x,y,marker='o', c='k')\n",
    "plt.plot(x_line, model(x_line),c='r')\n",
    "plt.xlabel('x'); plt.ylabel('y');\n",
    "plt.xlim([0,2.3]); plt.ylim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 40\n",
    "b = 2\n",
    "x_line = np.linspace(0,100,100)\n",
    "plt.scatter(x,y,marker='o', c='k')\n",
    "plt.plot(x_line, model(x_line),c='r')\n",
    "plt.xlabel('x'); plt.ylabel('y');\n",
    "plt.xlim([0,2.3]); plt.ylim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our notation, we will call $y(x)$ as $h_{a,b}(x)$ as the letter $h$ stands for **<i>hypothesis</i>**\n",
    "\n",
    "Today we have many data analysis frameworks that will do the job. In this exercise, we will use Linear Regression with gradient descent\n",
    "\n",
    "### Cost function\n",
    "\n",
    "Let's define a **cost function** as follows:\n",
    "\n",
    "$$ J(a,b) = \\frac{1}{2m}\\Sigma_i \\left( \\hat{y}_i - y_i \\right)^2$$\n",
    "\n",
    "where $ \\hat{y}_i = h_{a,b}(x) = ax_i + b $ and $m$ stands for total number of data points ($x_1,x_2,...,x_m$ and $y_1,y_2,...,y_m$)\n",
    "\n",
    "As one can see, the **cost function** defines some \"distance\" between the predicted model and the actual data points.\n",
    "\n",
    "In fitting problems, the idea is to minimize the cost function for $a$ and $b$ values.\n",
    "\n",
    "First, let us define the **cost function**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(a, b, x, y):\n",
    "\n",
    "    m = len(x)    \n",
    "    J = 0.5 * np.sum(np.square( a*x + b - y ) )  / m\n",
    "   \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lets look how the cost function looks as a function of the model parameters in range $a\\in(-1.5,2.5)$ and $b\\in(-5,35)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(x)        # m is the size of the sample\n",
    "n_points = 400    # number of points in (a,b) space to scan\n",
    "a_scan=np.expand_dims(np.linspace(-10, 90, num = n_points),axis=1)\n",
    "b_scan=np.expand_dims(np.linspace(-50, 50, num = n_points),axis=1)\n",
    "\n",
    "X,Y = np.meshgrid(a_scan, b_scan)\n",
    "J_scan = np.zeros((n_points,n_points))\n",
    "for i in range(n_points):\n",
    "    for j in range(n_points):\n",
    "        J_scan[i,j]=cost(X[i,j],Y[i,j],x,y)\n",
    "\n",
    "plt.figure()\n",
    "levels = np.logspace(0.1,5,20)\n",
    "CS = plt.contour(X, Y, J_scan, levels=levels)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.xlabel('a'); plt.ylabel('b'); plt.title('Contour plot of the cost function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye it looks like for $a = 5$ and $b = 0$ we have minimal value of the cost function.\n",
    "\n",
    "Lets Draw two predictions as before, one for the $a_1 = 0$ and $b_1 = 200$ and one for $a_2 = 4$ and $b_2 = 50$, and compute the total cost:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a1 = 0; b1 = 50;\n",
    "a2 = 40; b2 = 2;\n",
    "plt.scatter(x,y,marker='o', c='k')\n",
    "plt.plot(x,a1*x+b1,'r',label='y(x) = 0x + 50')\n",
    "plt.plot(x,a2*x+b2,'m',label='y(x) = 40x + 2')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.legend(fontsize=16)\n",
    "plt.show()\n",
    "print('for a=0 and b=50 we compute J(a,b) = ', cost(a1,b1,x,y))\n",
    "print('for a=40 and b=2 we compute J(a,b) = ', cost(a2,b2,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple example, minimizing the cost function can be done analytically by solving two equations:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial a}  = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b}  = 0 $$\n",
    "\n",
    "In practice (as we will see later) it is not always possible to calculate an exact solution.\n",
    "\n",
    "We will use here one of the powerful numerical methods to minimize the cost function\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "The gradient descent aglorithm is defined as follows:\n",
    "\n",
    "1. Compute partial derivatives of the cost function with respect to the model parameters: $\\frac{\\partial J}{\\partial a}$ and $\\frac{\\partial J}{\\partial b}$\n",
    "2. Initialize weight to some specific values $(a,b) = (a_0,b_0)$\n",
    "3. Define a **learning rate** parameter $\\alpha$\n",
    "4. Repeat $n_\\text{iteration}$ times (latter we will denote $n_\\text{iteration}$ as a number of **epoch**s)\n",
    "\n",
    "$$ \\theta_0^\\text{temp} = \\theta_0 - \\alpha\\times \\frac{\\partial J}{\\partial \\theta_0} $$ \n",
    "\n",
    "$$ \\theta_1^\\text{temp} = \\theta_1 - \\alpha\\times \\frac{\\partial J}{\\partial \\theta_1} $$ \n",
    "\n",
    "$$ \\theta_0 = \\theta_0^\\text{temp} $$ \n",
    "\n",
    "$$ \\theta_1 = \\theta_1^\\text{temp} $$ \n",
    "\n",
    "\n",
    "here we change the notation from $(a,b)$ to ($\\theta_0, \\theta_1$)\n",
    "\n",
    "\n",
    "### Partial derivative of the cost function:\n",
    "\n",
    "Using the cost function define by:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}\\Sigma_i \\left( \\theta_0 + \\theta_1x_i - y_i \\right)^2$$\n",
    "\n",
    "The partial derivative is given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial J}{\\partial \\theta_0} & = & \\frac{1}{m}\\Sigma_i \\left( \\theta_0 + \\theta_1x_i - y_i \\right) \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} & = & \\frac{1}{m}\\Sigma_i \\left( \\theta_0 + \\theta_1x_i - y_i \\right)x_i \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Let's define the cost function again, this time it will return the $J(\\theta)$ and $\\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "We will use vectorial representation as follows:\n",
    "$$ ax+b = \\theta_0 + \\theta_1x = \\begin{pmatrix}\\theta_0 &&  \\theta_1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1  \\\\ x \\end{pmatrix} $$\n",
    "\n",
    "Therefore we will update the input variable $x$ to $ [1 ;  x] $ and the weigts will be a column vector $\\vec{\\theta} = \\begin{pmatrix}\\theta_0 \\\\  \\theta_1 \\end{pmatrix}$\n",
    "\n",
    "Using this formalizm the prediction $\\hat{y}$ will be equal to $\\hat{y} = x\\cdot\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, x, y):\n",
    "    \n",
    "    m = len(x)\n",
    "    x = np.append(np.ones(x.shape),x, axis=1)\n",
    "    J = 0.5 * np.sum(np.square(np.matmul(x,weights) - y)) / m\n",
    "\n",
    "    grad = np.sum( (np.matmul(x,weights) - y) * x, axis=0, keepdims = True).T / m\n",
    "   \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementation of the gradient descent alrorithm\n",
    "\n",
    "Let's define the Gradient Descent function, with the following parameters:\n",
    "- data $(X,Y)$\n",
    "- initial weights (from now and on we will call denote the model parameters as weights)\n",
    "- learning rate\n",
    "- number of epochs\n",
    "\n",
    "```python\n",
    "def GradientDescent(X, Y, initial_weights, learning_rate, n_epoch):\n",
    "    # code to calculate weights\n",
    "    return weights, cache\n",
    "\n",
    "```\n",
    "We will also use a `cache` list to track the learning process of the algorithm (store the cost and the corresponding weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(X, Y, initial_weights, learning_rate, n_epoch):\n",
    "    weights=initial_weights\n",
    "    cache=[]\n",
    "    for i in range(n_epoch):\n",
    "        J, grad = cost(weights, X, Y)\n",
    "        cache.append((J,weights))\n",
    "        weights = weights - learning_rate*grad\n",
    "    J, _ = cost(weights, X, Y)\n",
    "    cache.append((J,weights))\n",
    "    return weights, cache    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the gradient descent algorithm for 200 iterations (epochs), learning rate $\\alpha=10^{-1}$ and look on the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.zeros((2,1))\n",
    "weights, cache = GradientDescent(x, y, initial_weights, learning_rate = 0.1, n_epoch = 50)\n",
    "print('after '+str(len(cache)-1)+' epochs the weights are a = '+str(weights[1])+' b = '+str(weights[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw again the contour plot as before, but we will add the updated weights for each of the 5 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = len(cache)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "levels = np.logspace(0.1,5,20)\n",
    "CS = plt.contour(X, Y, J_scan, levels=levels)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Contour plot of the cost function + updated weights for '+str(len(cache)-1)+' epochs')\n",
    "plt.xlabel(r\"a\",fontsize=20); plt.ylabel(r\"b\",fontsize=20)\n",
    "for i in range(nepoch):\n",
    "    plt.scatter(cache[i][1][1],cache[i][1][0],color='r')\n",
    "plt.subplot(122)\n",
    "xepoch=np.linspace(0,nepoch-1,nepoch)\n",
    "yepoch=np.zeros((nepoch,1))\n",
    "for i in range(nepoch): yepoch[i]=cache[i][0]\n",
    "plt.title('Evolution of the cost function')\n",
    "plt.ylabel(r\"$J(a,b)$\",fontsize=20); plt.xlabel('number of epochs',fontsize=20)\n",
    "plt.plot(xepoch,yepoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increase $\\alpha$ and to see what is happens. Why for $\\alpha=1$ the the algorithm diverges?\n",
    "Try to increase number of steps (epochs) to $200$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
