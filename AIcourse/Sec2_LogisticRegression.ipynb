{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Logistic Regression\n",
    "\n",
    "In this hands-on exercise, we will build an algorithm to identify handwritten digits. Before we design a complex Neural-Networks, we will start developing a simple algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwritten digits\n",
    "You will be working with a dataset that contains handwritten digits.\n",
    "\n",
    "Load the training Data, in this exercise we will use only small amount of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "\n",
    "(Xorig, Y),(_,_) = mnist.load_data()\n",
    "n=2000 # total amount of data to use\n",
    "Xorig=Xorig[:n]/255; Y=Y[:n]\n",
    "X = Xorig.reshape(Xorig.shape[0],28*28)\n",
    "print('Xorig.shape = ',Xorig.shape)\n",
    "print('X.shape = ',X.shape)\n",
    "print('Y.shape = ',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains 60000 examples of handwritten digits, we will use only small fraction of it - 2k images. This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/ ). Each example is $28\\times 28$ grayscale image of the digit. The 28 by 28 grid is \"unrolled\" into a 784-dimensional vector. Each example becomes a single row. This gives 2000 by 784 input matrix $X$.\n",
    "\n",
    "Let's visualize first $n$ digits, this is defined in function `displayData(data,n)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDigit(inputX,i):\n",
    "    xi = inputX[i]\n",
    "    return xi\n",
    "\n",
    "def displayData(X,Y,n,showtrue=True):\n",
    "    m=len(X)\n",
    "    inds=np.random.choice(m, n*n)\n",
    "    outX=np.zeros((n*28,n*28))\n",
    "    outY=np.zeros((n,n))\n",
    "    for i in range(n**2):\n",
    "        row_idx = i//n # equivalent to np.floor(i/n)\n",
    "        col_inx = i - n*row_idx\n",
    "        getDigit(X,[inds[i]]).shape\n",
    "        outX[28*(row_idx):28*(row_idx+1),28*(col_inx):28*(col_inx+1)] = getDigit(X,[inds[i]])\n",
    "        outY[row_idx,col_inx]=Y[inds[i]]\n",
    "    plt.imshow(outX)   \n",
    "    if showtrue:\n",
    "        print('The labels are:')\n",
    "        print(' '+np.array2string(outY.astype(int), precision=2, separator=' ')[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next cell to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 10  # to display nxn matrix of n*n random digits from the input data\n",
    "displayData(Xorig,Y,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be the identification of the handwritten digits!\n",
    "\n",
    "Before developing the algorithm, let's take a look on the average value of pixels\n",
    "\n",
    "Lets draw a scatter plot, for two types of digits $1$ and $8$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filter first n digits with Y=1 and 8\n",
    "n=10\n",
    "X1=Xorig[np.where(Y==1)][:n*n]\n",
    "X8=Xorig[np.where(Y==8)][:n*n]\n",
    "Y1=Y[np.where(Y==1)][:n*n]\n",
    "Y8=Y[np.where(Y==8)][:n*n]\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(221)\n",
    "displayData(X1,Y1,n,False)\n",
    "plt.subplot(222)\n",
    "displayData(X8,Y8,n,False)\n",
    "plt.subplot(223)\n",
    "plt.hist(np.sum(X1,axis=(1,2))/784,100,(0,0.25),label='mean = %2.2f'%np.mean(np.sum(X1,axis=(1,2))/784))\n",
    "plt.subplot(223).legend(); plt.xlabel('pixel average');\n",
    "plt.subplot(224)\n",
    "plt.hist(np.sum(X8,axis=(1,2))/784,100,(0,0.25),label='mean = %2.2f'%np.mean(np.sum(X8,axis=(1,2))/784))\n",
    "plt.subplot(224).legend();  plt.xlabel('pixel average');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find a function that can distinguish between two digits?\n",
    "\n",
    "Up to now, we considered a simple problem of fitting a linear function $y = ax + b$\n",
    "\n",
    "Here we are dealing a clasification problem - result is **Yes/No** (where $y = 1$ is positive class, and $y = 0$ is negative class).\n",
    "\n",
    "The output label, then will have next form $y =  \\left\\{ \\begin{eqnarray} 1 & \\text{ for } x=N \\\\ 0 & \\text{ for } x\\neq N \\\\ \\end{eqnarray} \\right. $\n",
    "\n",
    "The output function $h_\\theta(x)$ should be in the region between $0$ and $1$.\n",
    "\n",
    "### Using \"Logistic function\" as  an activation function\n",
    "\n",
    "We need to find a function, such that the output will be in the range between 0 and 1.\n",
    "\n",
    "A good candidate is \"logistic function\" or \"sigmoid function\" (https://en.wikipedia.org/wiki/Sigmoid_function). \n",
    "\n",
    "<br>\n",
    "<center><font size=\"5\">  $g(z) = \\frac{1}{1 + e^{-z}}$ </font> </center> \n",
    "\n",
    "Let's define and visualize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, num=100)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y,'r-')\n",
    "plt.ylabel('logistic function: g(z)',fontsize=20); plt.xlabel('z',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic cost function\n",
    "\n",
    "As we did in section 1, let's define a cost function in the following way:\n",
    "\n",
    "<center><font size=\"5\">  \n",
    "$ J(\\theta)  =  Cost(h_\\theta(x),y) = \\left\\{ \\begin{eqnarray} -\\log(h_\\theta(x)) & \\text{ if } y = 1 \\\\ -\\log(1 - h_\\theta(x)) & \\text{ if } y = 0 \\\\ \\end{eqnarray} \\right. $\n",
    "</font> </center> \n",
    "\n",
    "Let's draw it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,-np.log(y),'r-',y,-np.log(1-y),'b-')\n",
    "plt.legend(['-log(h)','-log(1-h)']); plt.xlabel(r'$h_\\theta (\\theta\\cdot x)$')\n",
    "plt.ylabel(r'J($\\theta$)'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the learning (gradient descent) process to minimize the cost function, we should compute derivatives of the cost with respect to $\\theta$. Fortunately, with the current definition of the cost function, the gradient has a simple form of:\n",
    "\n",
    "<center><font size=\"5\">  \n",
    "$ \\frac{dJ}{d\\theta_j} = \\frac{dJ(\\theta | x,y)}{d\\theta_j} = \\left(h_\\theta(x) - y\\right)x_j $\n",
    "</font> </center> \n",
    "\n",
    "Lets define both of them in `CostFunction()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunction(theta, X, Y):\n",
    "    \"\"\"\n",
    "    Implementation of the cost function for logistic regresion\n",
    "    \n",
    "    Arguments:\n",
    "    theta -- input vector of weights of shape (nf+1, 1)\n",
    "    intputX -- input data matrix of shape (m,n)\n",
    "    Y -- input labels of the data of shape (m,1) that contains {0,1}\n",
    "    \n",
    "    Returns:\n",
    "    J -- cost function for given input data and given weights\n",
    "    grad - vector of the gradients of the cost function with respect to each weight (shape - nf+1,1)\n",
    "    \"\"\"    \n",
    "    invm = 1 / Y.shape[0]\n",
    "    Xtmp = np.insert(X,0,1,axis=1)\n",
    "    Xt = np.dot(Xtmp,theta)\n",
    "    ht = sigmoid(Xt)\n",
    "    J = -invm*(np.dot(Y.T,np.log(ht)) + np.dot(1-Y.T,np.log(1-ht)))\n",
    "    #grad = np.sum((ht - Y)*Xtmp*invm,axis=0,keepdims=True)\n",
    "    grad = invm*np.dot((ht - Y).T,Xtmp).T\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the gradient descent algorithm, by computing gradients for several iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(initial_weights, X, Y, epoch = 10, learning_rate=0.001, vebrose=True):\n",
    "    \n",
    "    theta = initial_weights\n",
    "    Jvec=np.zeros((epoch,1))\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        J, grad = CostFunction(theta,X,Y)\n",
    "        Jvec[i]=J\n",
    "        \n",
    "        theta = theta - learning_rate*grad\n",
    "        if vebrose:\n",
    "            print('iteration n = ',i,'Cost funcion = ',J)\n",
    "        \n",
    "    return theta,Jvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's validate that the implimination is exact. For that propuse, we can compute gradient numerically for a given input vector $\\vec{\\theta}$ using the formula\n",
    "\n",
    "$$ \\frac{dJ}{d\\theta_j} = \\frac{J(\\theta + \\varepsilon_j) - J(\\theta - \\varepsilon_j)}{2|\\varepsilon|} $$ \n",
    "\n",
    "where $\\vec{\\varepsilon_j} = (0,0,...,\\varepsilon,0,...,0)$, with non zero value on entry $j^\\text{th}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_theta(m,n):\n",
    "    return np.random.rand(n+1,1)/float(m)\n",
    "    \n",
    "\n",
    "def CheckGradient(X,Y):\n",
    "    m, nf = X.shape\n",
    "    epsilon=1e-5\n",
    "    theta=initialize_theta(m,nf)\n",
    "    grad_diff=np.zeros((nf+1,1))\n",
    "    th,grad = CostFunction(theta,X,Y)\n",
    "    for i in range(nf+1):\n",
    "        theta_plus=np.copy(theta); theta_minus=np.copy(theta);\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        Jplus,grp = CostFunction(theta_plus,X,Y)\n",
    "        Jminus,grm = CostFunction(theta_minus,X,Y)\n",
    "        dJ = 0.5*(Jplus - Jminus)/epsilon\n",
    "        grad_diff[i]=dJ\n",
    "        if i<50 and grad_diff[i]:\n",
    "            print('check gradient',i,': numerical = ',grad_diff[i],', computed = ',grad[i],' diff/eps=',(grad_diff[i]-grad[i])/grad_diff[i])\n",
    "    #grad_diff = grad_diff - grad\n",
    "    #print(grad)\n",
    "    #print(grad_diff)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the gradient descent is implemented properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4 = np.zeros((len(Y),1));\n",
    "y4[np.where(Y==4)]=1\n",
    "CheckGradient(X,y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined a model, let's train the algorithm to identify a specific digit. We will change the representation of the labeled data $Y$ to be $\\{0,1\\}$ if it equals the correct number. Let's compute for $y=4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights\n",
    "theta = initialize_theta(X.shape[0],X.shape[1])\n",
    "\n",
    "#Train model to get the trained weights for different learning rate:\n",
    "print('Traning for learning rate = 1')\n",
    "[theta4lr1,Jlr1] = TrainModel(theta,X,y4,1000,1,False)\n",
    "print('Traning for learning rate = 0.1')\n",
    "[theta4lr2,Jlr2] = TrainModel(theta,X,y4,1000,0.1,False)\n",
    "print('Traning for learning rate = 0.01')\n",
    "[theta4lr3,Jlr3] = TrainModel(theta,X,y4,1000,0.01,False)\n",
    "print('Traning for learning rate = 0.001')\n",
    "[theta4lr4,Jlr4] = TrainModel(theta,X,y4,1000,0.001,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the algorithm convergence for different training rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=range(0,1000)\n",
    "plt.plot(iteration,Jlr1[iteration],'b',iteration,Jlr2[iteration],'g',iteration,Jlr3[iteration],'r',iteration,Jlr4[iteration],'c')\n",
    "plt.legend([r'$\\alpha$ = 1.000',r'$\\alpha$ = 0.100',r'$\\alpha$ = 0.010',r'$\\alpha$ = 0.001'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a trained vector $\\theta$, such that $h_{\\theta}(x)$ is maximized when $y = 4$\n",
    "\n",
    "Let's choose threshold value, first define Activation function, and prediction:\n",
    "\n",
    "- Activation function: $a(\\theta,X)=\\sigma\\left(X\\cdot\\theta\\right)\\in(0,1)$\n",
    "- prediction function: $ p(x_i) = \\left\\{ \\begin{eqnarray} 1 & \\text{ if } a(\\theta,x_i) > \\text{threshold} \\\\ 0 & \\text{ otherwise } \\\\ \\end{eqnarray} \\right. $\n",
    "- accuracy function: number of correctly predicted entries / total number of different entries (in %)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivation(theta,intputX):\n",
    "    m = intputX.shape[0]\n",
    "    Xtmp = np.insert(intputX,0,1,axis=1)\n",
    "    Xt = np.dot(Xtmp,theta)\n",
    "    return sigmoid(Xt)\n",
    "\n",
    "def predict(A,th):\n",
    "    m=A.shape[0]\n",
    "    y = np.zeros((m,1))\n",
    "    y[A > th] = 1\n",
    "    return y\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    m=y.shape[0]\n",
    "    return np.sum(yhat==y)/m*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy of the previously trained model (recall we obtain variable `theta4`), let's define a threshold of 0.5, and calculate how well we predict  digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = getActivation(theta4lr1,X)\n",
    "yhat = predict(y4,0.5)\n",
    "print('accuracy = ',accuracy(yhat,y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find a threshold, that maximizes accuracy for the given example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best=0\n",
    "acc_best=0\n",
    "threshold=np.arange(0.2, 1.0, 0.01)\n",
    "acc=np.zeros(threshold.shape)\n",
    "for i in range(threshold.shape[0]):\n",
    "    yhat = predict(A4,threshold[i])\n",
    "    acc[i] = accuracy(yhat,y4)\n",
    "    if acc_best<acc[i]: acc_best=acc[i]; threshold_best=threshold[i]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(threshold, acc)\n",
    "\n",
    "ax.set(xlabel='threshold (s)', ylabel='accuracy (%)',\n",
    "       title='Optimization of the threshold')\n",
    "ax.grid()\n",
    "print('best threshold is %2.2f that gives accuracy of %2.2f' % (threshold_best,acc_best)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rembmer, we have 10 digits, while we only trained the model for $y=4$. Let's repeat the exersize for all digits, and check if it improves the estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(X,Y,i):\n",
    "    m, nf = X.shape\n",
    "    theta = initialize_theta(m,nf)\n",
    "    yi=np.zeros((m,1)); yi[Y==i]=1\n",
    "    theta, _ = TrainModel(theta,X,yi,100,1,False)\n",
    "    A = getActivation(theta,X)\n",
    "    Yhat = predict(A,0.5)\n",
    "    accuracy = np.sum(Yhat==yi) / m * 100.0 # in percent\n",
    "    indx = np.where(Yhat!=yi)\n",
    "    return theta, Yhat, yi, A, accuracy, indx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlabels=10\n",
    "yN=[];Theta=[]; accuracy=[]; indx=[]; Yhat = []; Ai=[]\n",
    "for i in range(nlabels):\n",
    "    theta, Yh, yi, a, acc, ind = runModel(X,Y,i);\n",
    "    yN.append(yi)\n",
    "    Theta.append(theta)\n",
    "    accuracy.append(acc)\n",
    "    Ai.append(a)\n",
    "    indx.append(ind[0])\n",
    "    Yhat.append(Yh)\n",
    "    print('done for',i,' accuracy = ',acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see high accuracy for training separately each one of the digits. In the combined method, we can prob, which of the digits has the highest probability (highest activation, softmax activation).\n",
    "\n",
    "Let loon on a specific example where the algorithm failed to identify correctly digit 4:\n",
    "the indeces of incorrectly identified digits are stored in list `indx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongindex = np.random.choice(indx[4],1)[0]\n",
    "print('The correct gidit is '+str(Y[wrongindex]))\n",
    "for i in range(10):\n",
    "    print('for digit = '+str(i)+' the value, the predicted activation value is = ',Ai[i][wrongindex])\n",
    "\n",
    "plt.imshow(getDigit(Xorig,wrongindex))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "In this section, we will implement a basic neural network to identify digits. Although logistic regression does a pretty good job, we will see in the further example, that for more sophisticated image classification, logistic regression will be not that precise.\n",
    "\n",
    "Until now we define the task as follows:\n",
    "- for data $X$ and labels $Y$, we defined a function $F(X,\\theta)$, where $\\theta$ are free parameters of the problem\n",
    "- To find the optimal function, we defined a cost function $J(\\theta)$ that holds $\\forall x$ $F(x,\\theta)\\approx Y$\n",
    "- we used gradient descent to minimize the function with respect to free parameters $\\theta$\n",
    "- This process is called \"learning process of the ML algorithm\"\n",
    "\n",
    "Today, there are existing packages, that do this job efficiently and we use the one for this propose.\n",
    "\n",
    "### Keras\n",
    "\n",
    "Programing with keras is very easy. First we need to load relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "\n",
    "#visualization of the keras model\n",
    "from IPython.display import Image\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Building a model\n",
    "\n",
    "Keras is very good for rapid prototyping. In just a short time you will be able to build a model that achieves outstanding results.\n",
    "\n",
    "Here is an example of a model in Keras:\n",
    "\n",
    "```python\n",
    "def model(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Make activation function for the input layer\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X_input)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='MyFirstModel')\n",
    "    \n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyFirstModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the MyFirstModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape, name='Image')\n",
    "    \n",
    "    # DEnse X creates a FULLYCONNECTED layer\n",
    "    X = Dense(1, activation='sigmoid', name='OutputLayer')(X_input)   \n",
    "    \n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='MyFirstModel')    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now built a function to describe your model. To train and test this model, there are four steps in Keras:\n",
    "1. Create the model by calling the function above\n",
    "2. Compile the model by calling `model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])`\n",
    "3. Train the model on train data by calling `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)`\n",
    "4. Test the model on test data by calling `model.evaluate(x = ..., y = ...)`\n",
    "\n",
    "If you want to know more about `model.compile()`, `model.fit()`, `model.evaluate()` and their arguments, refer to the official [Keras documentation](https://keras.io/models/model/).\n",
    "\n",
    "1. Creating a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyFirstModel = MyFirstModel((X.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compile the model to configure the learning process. Choose the 3 arguments of `compile()` wisely. Here we will use \"ADAM\" (adaptive moment) optimizer, which is a more advanced version of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "MyFirstModel.compile(optimizer = adam, loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model. Choose the number of epochs and the batch size. Train the example only for $y=4$\n",
    "\n",
    "For training use only half of the data ($n=1500$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniX=X[:1500]; miniY=Y[:1500]\n",
    "y4=np.zeros((miniX.shape[0],1)); y4[miniY==4]=1\n",
    "history = MyFirstModel.fit(x = miniX,y = y4, epochs = 100, batch_size = 128, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you run `fit()` again, the `model` will continue to train with the parameters it has already learnt instead of reinitializing them.\n",
    "\n",
    "4. Test/evaluate the model.\n",
    "The testing will be done on a sample that the model didn't saw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select new dataset:\n",
    "miniX_test=X[1500:]; miniY_test=Y[1500:]\n",
    "#develop the model for Y=4, therefore change labels to 1 or 0 if Y=4 of else respectively \n",
    "y4_test = np.zeros((500,1));\n",
    "y4_test[np.where(miniY_test==4)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = MyFirstModel.evaluate(x = miniX, y = y4, batch_size=miniX.shape[1])\n",
    "preds_test = MyFirstModel.evaluate(x = miniX_test, y = y4_test, batch_size=miniX_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss(train) = \" + str(preds_train[0])+\", Loss(test) = \" + str(preds_test[0]))\n",
    "print(\"Train Accuracy = \" + str(preds_train[1])+\" Test Accuracy = \" + str(preds_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful functions in Keras\n",
    "\n",
    "Two other basic features of Keras that you'll find useful are:\n",
    "- `model.summary()`: prints the details of your layers in a table with the sizes of its inputs/outputs\n",
    "- `plot_model()`: plots your graph in a nice layout. You can even save it as \".png\" using SVG() if you'd like to share it on social media ;). It is saved in \"File\" then \"Open...\" in the upper bar of the notebook.\n",
    "\n",
    "Run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyFirstModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(MyFirstModel, to_file='MyFirstModel.png', show_shapes=True, show_layer_names=False)\n",
    "Image(\"MyFirstModel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
